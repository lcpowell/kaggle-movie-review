{
 "metadata": {
  "name": "",
  "signature": "sha256:4a07c68e6ad93485876cfc713ec050d389d37bfcf38fc319115c3b954c88f69c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Walk-through of a simple solution to Kaggle's 'Sentiment Analysis on Movie Reviews' competition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Examine the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "pd.set_option('display.max_rows',10)\n",
      "pd.set_option('display.max_columns',10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use pandas to read in the training and test data, which was downloaded from the [Kaggle competition Data page](http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) as tsv files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train=pd.read_csv('train.tsv',sep='\\t',encoding='utf_8')\n",
      "test=pd.read_csv('test.tsv',sep='\\t',encoding='utf_8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have an initial look at the first sentence to see how sentences are split into phrases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train[train.SentenceId==1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PhraseId</th>\n",
        "      <th>SentenceId</th>\n",
        "      <th>Phrase</th>\n",
        "      <th>Sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>  1</td>\n",
        "      <td> 1</td>\n",
        "      <td> A series of escapades demonstrating the adage ...</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>  2</td>\n",
        "      <td> 1</td>\n",
        "      <td> A series of escapades demonstrating the adage ...</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>  3</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                          A series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>  4</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                                 A</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>  5</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                            series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>58</th>\n",
        "      <td> 59</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                              much</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>59</th>\n",
        "      <td> 60</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                        of a story</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>60</th>\n",
        "      <td> 61</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                           a story</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>61</th>\n",
        "      <td> 62</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                             story</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>62</th>\n",
        "      <td> 63</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                                 .</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>63 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "    PhraseId  SentenceId                                             Phrase  \\\n",
        "0          1           1  A series of escapades demonstrating the adage ...   \n",
        "1          2           1  A series of escapades demonstrating the adage ...   \n",
        "2          3           1                                           A series   \n",
        "3          4           1                                                  A   \n",
        "4          5           1                                             series   \n",
        "..       ...         ...                                                ...   \n",
        "58        59           1                                               much   \n",
        "59        60           1                                         of a story   \n",
        "60        61           1                                            a story   \n",
        "61        62           1                                              story   \n",
        "62        63           1                                                  .   \n",
        "\n",
        "    Sentiment  \n",
        "0           1  \n",
        "1           2  \n",
        "2           2  \n",
        "3           2  \n",
        "4           2  \n",
        "..        ...  \n",
        "58          2  \n",
        "59          2  \n",
        "60          2  \n",
        "61          2  \n",
        "62          2  \n",
        "\n",
        "[63 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The phrases **include** stopwords and punctutation in combination with others words **and** individually, so we need to be able to predict sentiment values for these too. This suggests we may want to skip the usual step of removing all stopwords."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Try basic tokenization and Naive Bayes Classifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start with the quickest/simplest possible solution. Naive Bayes models are usually fast and can perform well on text classification tasks.\n",
      "\n",
      "We will just use single words as tokens and will not remove stopwords (this is the default behaviour of CountVectorizer, which is described below)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "a) Bernoulli Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start with basic BernoulliNB classifier using word occurences as features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CountVectorizer will tokenize the data and create a sparse matrix, where the tokens are the features. The simplest option is to use word occurences as features (i.e. feature has value 1 if that token occurs in the phrase, 0 if not). We set binary=True to use this.\n",
      "\n",
      "The BernoulliNB classifier is appropriate for boolean features. \n",
      "\n",
      "Train the classifier and use cross-validation to estimate how accurate our classifier is (using 3 folds of training data)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv_bool=CountVectorizer(binary=True)\n",
      "xtrain_bool=cv_bool.fit_transform(train.Phrase)\n",
      "xtest_bool=cv_bool.transform(test.Phrase)\n",
      "\n",
      "clf_bool=BernoulliNB().fit(xtrain_bool,train.Sentiment)\n",
      "\n",
      "cross_validation.cross_val_score(clf_bool, xtrain_bool, train.Sentiment,  cv=3, n_jobs=-1).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.5398498507685322"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Predict sentiment for phrases in test data and ouput to file for submission to Kaggle for scoring."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ypred_bool=clf_bool.predict(xtest_bool)\n",
      "\n",
      "pd.DataFrame({'PhraseId':test.PhraseId,'Sentiment':ypred_bool},\n",
      "             columns=['PhraseId','Sentiment']).to_csv('submission_bool.csv',index=False,header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "b) Multinomial Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now try using word counts as features (instead of word occurences) with MulitnomialNB classifier. We need to use CountVectorizer again, without the binary option."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "cv_counts = CountVectorizer()\n",
      "xtrain_counts = cv_counts.fit_transform(train.Phrase)\n",
      "xtest_counts = cv_counts.transform(test.Phrase)\n",
      "\n",
      "clf_counts = MultinomialNB().fit(xtrain_counts, train.Sentiment)\n",
      "ypred_counts = clf_counts.predict(xtest_counts)\n",
      "\n",
      "pd.DataFrame({'PhraseId':test.PhraseId,'Sentiment':ypred_counts},\n",
      "              columns=['PhraseId','Sentiment']).to_csv('submission_counts.csv',index=False, header=True)\n",
      "\n",
      "cross_validation.cross_val_score(clf_counts, xtrain_counts, train.Sentiment,  cv=3, n_jobs=-1).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.54014463925553191"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "d) Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Very similar performance!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When evaluated on the test set by Kaggle, the best model is MultinomialNB with word counts, achieveing a score of 0.599 (only beating the Bernoulli model by $\\approx$ 0.0001). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Try Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try a different type of classifier. Support Vector Machines are known to perform well on text classification tasks.\n",
      "\n",
      "We have labeled data, are predicting categories and have >100K samples (there are 156060 training samples), so try the SGDClassifier first, as suggested by the [sklearn cheatsheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "a) SGDClassifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For SVMs the data needs to be scaled, so let's use TF-IDF for the features, which we can obtain from the word counts with TfidfTransformer.\n",
      "\n",
      "We'll create a pipeline object to make analysis easier by tying the preprocessing and the classifier together. We can then supply the original (unprocessed) data to the pipeline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "sgd_tfidf_pipe=Pipeline([('cvec',CountVectorizer()),('tfidf',TfidfTransformer()),\n",
      "                         ('clf',SGDClassifier(loss='hinge',shuffle=True))])\n",
      "\n",
      "sgd_tfidf_pipe.fit(train.Phrase,train.Sentiment)\n",
      "\n",
      "ypred_sgd_tfidf=sgd_tfidf_pipe.predict(test.Phrase)\n",
      "\n",
      "pd.DataFrame({'PhraseId':test.PhraseId,'Sentiment':ypred_sgd_tfidf},\n",
      "              columns=['PhraseId','Sentiment']).to_csv('submission_sgd_tfidf.csv',index=False, header=True)\n",
      "\n",
      "cross_validation.cross_val_score(sgd_tfidf_pipe,train.Phrase,train.Sentiment,cv=3,n_jobs=-1).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.55307575961501521"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks promising, but we haven't optimisied the model hyperparameters yet. Let's start to dig a bit deeper and find the best values for some of these."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "i) Setting the hyperparameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use GridSearchCV which uses cross-validation of the training data to evaulate all the combinations of parameter choices we supply. Parameters can be supplied for each stage in the pipeline. This (obviously) can take a long time, so we'll just select a few of the parameters we believe could be most important here.\n",
      "\n",
      "Let's test:\n",
      "\n",
      "* our initial assumption that we should leave the stopwords in when calling CountVectorizer.\n",
      "\n",
      "\n",
      "* if also extracting bigrams with CountVectorizer to use as features can improve performance.\n",
      "\n",
      "\n",
      "* whether using TF or TF-IDF in the TfidfTransformer performs better.\n",
      "\n",
      "\n",
      "* if varying the type of penalty, the number of iterations and alpha (which controls regularisation) for SGDClassifier is beneficial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "sgd_tfidf_params = {'cvec__ngram_range': [(1, 1), (1, 2)],\n",
      "                    'cvec__stop_words':('english',None),\n",
      "                    'tfidf__use_idf':(True,False),\n",
      "                    'clf__alpha': (1e-4,1e-5),\n",
      "                    'clf__penalty':('l2','elasticnet'),\n",
      "                    'clf__n_iter':(5,10,20)}\n",
      "\n",
      "gs_sgd_tfidf = GridSearchCV(sgd_tfidf_pipe, sgd_tfidf_params, n_jobs=-1)\n",
      "gs_sgd_tfidf.fit(train.Phrase,train.Sentiment)\n",
      "\n",
      "gs_sgd_tfidf.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.58976675637575293"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our best score has improved (on the training data). Lets make predicitions and write to file to submit to Kaggle, so we can evaluate the model against unseen data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_sgd_tfidf.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "{'clf__alpha': 1e-05,\n",
        " 'clf__n_iter': 10,\n",
        " 'clf__penalty': 'l2',\n",
        " 'cvec__ngram_range': (1, 1),\n",
        " 'cvec__stop_words': None,\n",
        " 'tfidf__use_idf': False}"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sgd_tfidf_pipe.set_params(clf__alpha=1e-5,clf__n_iter=10,clf__penalty='elasticnet',\n",
      "                          tfidf__use_idf=False,cvec__ngram_range=(1,2)).fit(train.Phrase,train.Sentiment)\n",
      "\n",
      "ypred_sgd_tfidf=sgd_tfidf_pipe.predict(test.Phrase)\n",
      "\n",
      "pd.DataFrame({'PhraseId':test.PhraseId,'Sentiment':ypred_sgd_tfidf},\n",
      "              columns=['PhraseId','Sentiment']).to_csv('submission_gs_sgd_tfidf.csv',index=False, header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have the same order of magnitude of samples as the maximum recommended number for trying LinearSVC, so let's see how that classifier performs too."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "b) LinearSVC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repeat the same procedure as above to choose the hyperparameters for LinearSVC.\n",
      "\n",
      "We set dual=False since n_samples > n_features (as recommended in the docs)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "lsvc_tfidf_pipe=Pipeline([('cvec',CountVectorizer()),('tfidf',TfidfTransformer()),\n",
      "                          ('clf',LinearSVC(dual=False))])\n",
      "\n",
      "lsvc_tfidf_params = {'cvec__ngram_range': [(1, 1), (1, 2)],\n",
      "                    'cvec__stop_words':('english',None),\n",
      "                    'tfidf__use_idf':(True,False),\n",
      "                    'clf__penalty':('l1','l2'),\n",
      "                    'clf__C':(0.1,0.3,1.0)}\n",
      "\n",
      "gs_lsvc_tfidf = GridSearchCV(lsvc_tfidf_pipe, lsvc_tfidf_params, n_jobs=-1)\n",
      "gs_lsvc_tfidf.fit(train.Phrase,train.Sentiment)\n",
      "\n",
      "gs_lsvc_tfidf.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.59749455337690627"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks like the best result yet."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train LinearSVC with the best parameters from grid search and write predicted sentiments to file to submit results to Kaggle for evaluation on the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_lsvc_tfidf.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "{'clf__C': 0.3,\n",
        " 'clf__penalty': 'l1',\n",
        " 'cvec__ngram_range': (1, 2),\n",
        " 'cvec__stop_words': None,\n",
        " 'tfidf__use_idf': False}"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc_tfidf_pipe.set_params(clf__penalty='l1',clf__C=0.3,tfidf__use_idf=False,cvec__ngram_range=(1,2)\n",
      "                           ).fit(train.Phrase,train.Sentiment)\n",
      "ypred_lsvc_tfidf=lsvc_tfidf_pipe.predict(test.Phrase)\n",
      "\n",
      "pd.DataFrame({'PhraseId':test.PhraseId,'Sentiment':ypred_lsvc_tfidf},\n",
      "              columns=['PhraseId','Sentiment']).to_csv('submission_gs_lsvc_tfidf.csv',index=False, header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "c) Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SGDClassifier model scored 0.62 and the LinearSVC model scored 0.63 when evaluated on the test data by Kaggle."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Evaluation of the models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on the Kaggle public scores, the LinearSVC model is the most accurate of the 4 classifiers we tried. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Ideas for future work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model chosen above obtains a score of 0.63 on the Kaggle public leaderboard, compared to the current first place score of 0.76. Given the simplicity and speed of this model, this performance seems reasonable.\n",
      "\n",
      "There are, however, several possible improvements to investigate further:\n",
      "\n",
      "* **Modifiying the tokenization so that it more closely matches how the sentences are broken down into phrases in the data.** \n",
      "The model currently uses the default tokenization of CountVectorizer, which strips the punctuation and parses the phrases using word/nonword boundaries. The phrases that have been rated for sentiment in the data include punctuation and this sometimes changes the sentiment score. I would like to investigate to see if better matching of the (one token) phrases in the data to the tokenization used to create features can improve the model.\n",
      "\n",
      "\n",
      "* **Removing features that do not contain much information.** \n",
      "The model currently uses all the words/tokens as features. It would be interesting to see if selecting only the top x% of features improves the model's accuracy.\n",
      "\n",
      "\n",
      "* **Stemming the words**\n",
      "\n",
      "\n",
      "* **Adding new features based on other properties of the phrases** e.g. number of tokens/words\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}